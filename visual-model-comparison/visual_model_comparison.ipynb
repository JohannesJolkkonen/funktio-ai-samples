{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Vision Models\n",
    "## **GPT4-V** vs **Gemini Pro** vs **Claude 3** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "import dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Encodes an image to a base64 string.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def call_gpt4_vision(image_path, question):\n",
    "    base64_image = (encode_image(image_path),)\n",
    "    start = timer()\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64m {base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    latency = timer() - start\n",
    "    tokens = response.usage.total_tokens\n",
    "    content = response.choices[0].message.content\n",
    "    return content, latency, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projected revenue for 2018, according to the provided image, is $4,800,000. The image indicates that the break-even point is reached in the third year, which would be 2017. Given this information, we can infer that the company is projected to be profitable in 2018, as it is expected to surpass the break-even point by achieving the $4,800,000 revenue mark.\n"
     ]
    }
   ],
   "source": [
    "img_path = \"data/financial_projections.png\"\n",
    "question = \"What is our projected revenue for 2018, and are we profitable?\"\n",
    "\n",
    "answer = call_gpt4_vision(img_path, question)[0] \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel, Part, Image\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project='your-project-name', location='europe-west1')\n",
    "gemini_vision = GenerativeModel(\"gemini-1.0-pro-vision\")\n",
    "\n",
    "def call_gemini_vision(image_path, question):\n",
    "    image_part = Part.from_image(Image.load_from_file(image_path))\n",
    "    question_part = Part.from_text(question)\n",
    "    start = timer()\n",
    "    response = gemini_vision.generate_content([question_part, image_part])\n",
    "    latency = timer() - start \n",
    "    tokens = response.to_dict()['usage_metadata']['total_token_count']\n",
    "    try:\n",
    "        content = response.candidates[0].content.parts[0].text\n",
    "        return content, latency, tokens\n",
    "    except Exception as e:\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Latent Dirichlet allocation (LDA)\n"
     ]
    }
   ],
   "source": [
    "question = \"What concept is explained in this diagram?\"\n",
    "img_path = \"data/lda_model.png\"\n",
    "\n",
    "answer = call_gemini_vision(img_path, question)[0] \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Claude 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from time import sleep\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "\n",
    "def call_claude3_vision(image_path, question, model=\"claude-3-opus-20240229\"):\n",
    "    base64_image = encode_image(image_path)\n",
    "    image_type = image_path.split('.')[-1].replace('jpg', 'jpeg')\n",
    "    start = timer()\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": f\"image/{image_type}\",\n",
    "                            \"data\": base64_image\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    latency = timer() - start \n",
    "    message = response.content[0].text\n",
    "    tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "    sleep(8)\n",
    "    return message, latency, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This diagram appears to be explaining the process of topic modeling, which is a technique used to discover the abstract \"topics\" that occur in a collection of text documents. The diagram shows the main steps involved, including the collection of text documents, the calculation of a document-topic distribution, the assignment of words to topics, and the final creation and analysis of the topic frequencies across the documents.\n"
     ]
    }
   ],
   "source": [
    "question = \"What concept is explained in this diagram?\"\n",
    "img_path = \"data/lda_model.png\"\n",
    "\n",
    "# Models = \"claude-3-haiku-20240307\", \"claude-3-sonnet-20240229\"\n",
    "\n",
    "answer = call_claude3_vision(img_path, question, model=\"claude-3-haiku-20240307\")[0] \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the results with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_correctness\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "class ImageQA:\n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            \"images\": [\n",
    "                \"data/financial_projections.png\",\n",
    "                \"data/soda_nutrition.jpeg\",\n",
    "                \"data/actors_graph.png\",\n",
    "                \"data/lex_yann.jpg\",\n",
    "                \"data/lda_model.png\",\n",
    "                \"data/amazon_items.png\",\n",
    "                \"data/license_plate.jpeg\",\n",
    "                \"data/street_night.jpg\",\n",
    "                \"data/contrastive_learning_2.png\"\n",
    "            ],\n",
    "            \"questions\": [\n",
    "                \"What is our projected revenue for 2018, and are we profitable?\",\n",
    "                \"Give me the complete nutritional facts, grams and % daily value\",\n",
    "                \"Which two actors had the most movies in common?\",\n",
    "                \"Who are the two famous people in this image?\",\n",
    "                \"What concept is explained in this diagram?\",\n",
    "                \"What is the cheapest item, and the most expensive item in this picture?\",\n",
    "                \"What's the license of the main car in the image?\",\n",
    "                \"What does the text say in the longest billboard in the image?\",\n",
    "                \"What concept is explained in this diagram?\"\n",
    "            ],\n",
    "            \"ground_truths\": [\n",
    "                \"The projected revenue for 2018 is $4,800,000. The image includes a break-even point at the 3rd year, indicating that the company is projected to be profitable by 2018.\",\n",
    "                \"Calories 25\\nTotal Fat 0g 0%\\nSodium 0mg 0%\\nTotal Carbohydrate 7g 3%\\nDietary Fiber 2g 7%\\nTotal Sugars 5g 8%\\nProtein 0g 0%\",\n",
    "                \"The two actors who had the most movies in common were Keanu Reeves and Laurence Fishburne. They both starred in four movies together: The Matrix, The Matrix Reloaded and The Matrix Revolutions\",\n",
    "                \"Lex Fridman, a podcaster, and Yann Lecun, an AI scientist\",\n",
    "                \"Latent Dirichlet Allocation (LDA)\",\n",
    "                \"The cheapest item in the picture is the 'Renova Rollo de Cocina, 3 Unidades (Paquete de 1)' for 3,45 â‚¬. The most expensive item in the picture is the 'ProteÃ­na Sin Lactosa de HSN | Sin Sabor 500 g' for 22,90 â‚¬.\",\n",
    "                \"The license is 'CL10760'.\",\n",
    "                \"The billboard reads 'Madame Tussauds'.\",\n",
    "                \"The diagram describes 'triplet loss', a technique used to train a neural network to group similar images together and separate dissimilar ones. It does this by minimizing the distance between images of the same identity and maximizing the distance between images of different identities.\"\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        self.metrics = [answer_correctness]\n",
    "\n",
    "    def image_qa(self, image_path, question, model):\n",
    "        print(f\"Running visual QA for: {image_path}\")\n",
    "        if model == 'gpt4':\n",
    "            return call_gpt4_vision(image_path, question)\n",
    "        elif model == 'gemini':\n",
    "            return call_gemini_vision(image_path, question)\n",
    "        elif model == 'claude-opus':\n",
    "            return call_claude3_vision(image_path, question)\n",
    "        elif model == 'claude-haiku':\n",
    "            return call_claude3_vision(image_path, question, model=\"claude-3-haiku-20240307\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid model name\")\n",
    "\n",
    "\n",
    "    def run_evaluation(self, model):\n",
    "        answers = []\n",
    "        latencies = []\n",
    "        token_usage = []\n",
    "\n",
    "        for index, question in enumerate(self.data['questions']):\n",
    "            image_path = self.data['images'][index]\n",
    "            response, latency, tokens = self.image_qa(image_path, question, model)\n",
    "\n",
    "            answers.append(response)\n",
    "            latencies.append(latency)\n",
    "            token_usage.append(tokens)\n",
    "\n",
    "\n",
    "        ragas_testset = Dataset.from_dict(\n",
    "            {\n",
    "                \"question\": self.data['questions'],\n",
    "                \"ground_truth\": self.data['ground_truths'],\n",
    "                \"answer\": answers,\n",
    "                \"latencies\": latencies,\n",
    "                \"token_usage\": token_usage,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        eval_results = evaluate(ragas_testset, metrics=self.metrics)\n",
    "        df = eval_results.to_pandas()\n",
    "        df.to_csv(f\"eval_results/{model}_evals.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running visual QA for: data/financial_projections.png\n",
      "Running visual QA for: data/soda_nutrition.jpeg\n",
      "Running visual QA for: data/actors_graph.png\n",
      "Running visual QA for: data/lex_yann.jpg\n",
      "Running visual QA for: data/lda_model.png\n",
      "Running visual QA for: data/amazon_items.png\n",
      "Running visual QA for: data/license_plate.jpeg\n",
      "Running visual QA for: data/street_night.jpg\n",
      "Running visual QA for: data/contrastive_learning_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedcb0a746844d95922bcafde3a01400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-22' coro=<AsyncClient.aclose() done, defined at c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpx\\_client.py:1996> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 2003, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 383, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 252, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"c:\\Users\\JohannesJolkkonen\\Documents\\Code\\gettingdata-samples\\visual-model-comparison\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1181, in aclose\n",
      "    self._transport.close()\n",
      "  File \"C:\\Users\\JohannesJolkkonen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\selector_events.py\", line 831, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"C:\\Users\\JohannesJolkkonen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 758, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"C:\\Users\\JohannesJolkkonen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "image_qa = ImageQA()\n",
    "\n",
    "image_qa.run_evaluation(model=\"gpt4\")\n",
    "image_qa.run_evaluation(model=\"gemini\")\n",
    "image_qa.run_evaluation(model=\"claude-opus\")\n",
    "image_qa.run_evaluation(model=\"claude-haiku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "      <th>latencies</th>\n",
       "      <th>token_usage</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is our projected revenue for 2018, and ar...</td>\n",
       "      <td>The projected revenue for 2018 is $4,800,000. ...</td>\n",
       "      <td>The projected revenue for 2018 is $4,800,000....</td>\n",
       "      <td>2.932736</td>\n",
       "      <td>317</td>\n",
       "      <td>0.613849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Give me the complete nutritional facts, grams ...</td>\n",
       "      <td>Calories 25\\nTotal Fat 0g 0%\\nSodium 0mg 0%\\nT...</td>\n",
       "      <td>**Nutrition Facts**\\n\\nServing Size: 1 Can (1...</td>\n",
       "      <td>3.911964</td>\n",
       "      <td>393</td>\n",
       "      <td>0.919710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which two actors had the most movies in common?</td>\n",
       "      <td>The two actors who had the most movies in comm...</td>\n",
       "      <td>The two actors who had the most movies in com...</td>\n",
       "      <td>3.091800</td>\n",
       "      <td>313</td>\n",
       "      <td>0.744702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who are the two famous people in this image?</td>\n",
       "      <td>Lex Fridman, a podcaster, and Yann Lecun, an A...</td>\n",
       "      <td>The two famous people in the image are:\\n\\n- ...</td>\n",
       "      <td>2.854356</td>\n",
       "      <td>298</td>\n",
       "      <td>0.518296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What concept is explained in this diagram?</td>\n",
       "      <td>Latent Dirichlet Allocation (LDA)</td>\n",
       "      <td>Latent Dirichlet Allocation (LDA)</td>\n",
       "      <td>2.866267</td>\n",
       "      <td>272</td>\n",
       "      <td>0.988251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the cheapest item, and the most expens...</td>\n",
       "      <td>The cheapest item in the picture is the 'Renov...</td>\n",
       "      <td>The cheapest item is the \"Renova Rollo de Coc...</td>\n",
       "      <td>3.878212</td>\n",
       "      <td>336</td>\n",
       "      <td>0.993904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What's the license of the main car in the image?</td>\n",
       "      <td>The license is 'CL10760'.</td>\n",
       "      <td>The license of the main car in the image is C...</td>\n",
       "      <td>3.328562</td>\n",
       "      <td>288</td>\n",
       "      <td>0.717002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What does the text say in the longest billboar...</td>\n",
       "      <td>The billboard reads 'Madame Tussauds'.</td>\n",
       "      <td>Madame Tussauds</td>\n",
       "      <td>2.775198</td>\n",
       "      <td>276</td>\n",
       "      <td>0.974335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What concept is explained in this diagram?</td>\n",
       "      <td>The diagram describes 'triplet loss', a techni...</td>\n",
       "      <td>The diagram explains the concept of contrasti...</td>\n",
       "      <td>5.207717</td>\n",
       "      <td>582</td>\n",
       "      <td>0.688275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is our projected revenue for 2018, and ar...   \n",
       "1  Give me the complete nutritional facts, grams ...   \n",
       "2    Which two actors had the most movies in common?   \n",
       "3       Who are the two famous people in this image?   \n",
       "4         What concept is explained in this diagram?   \n",
       "5  What is the cheapest item, and the most expens...   \n",
       "6   What's the license of the main car in the image?   \n",
       "7  What does the text say in the longest billboar...   \n",
       "8         What concept is explained in this diagram?   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The projected revenue for 2018 is $4,800,000. ...   \n",
       "1  Calories 25\\nTotal Fat 0g 0%\\nSodium 0mg 0%\\nT...   \n",
       "2  The two actors who had the most movies in comm...   \n",
       "3  Lex Fridman, a podcaster, and Yann Lecun, an A...   \n",
       "4                  Latent Dirichlet Allocation (LDA)   \n",
       "5  The cheapest item in the picture is the 'Renov...   \n",
       "6                          The license is 'CL10760'.   \n",
       "7             The billboard reads 'Madame Tussauds'.   \n",
       "8  The diagram describes 'triplet loss', a techni...   \n",
       "\n",
       "                                              answer  latencies  token_usage  \\\n",
       "0   The projected revenue for 2018 is $4,800,000....   2.932736          317   \n",
       "1   **Nutrition Facts**\\n\\nServing Size: 1 Can (1...   3.911964          393   \n",
       "2   The two actors who had the most movies in com...   3.091800          313   \n",
       "3   The two famous people in the image are:\\n\\n- ...   2.854356          298   \n",
       "4                  Latent Dirichlet Allocation (LDA)   2.866267          272   \n",
       "5   The cheapest item is the \"Renova Rollo de Coc...   3.878212          336   \n",
       "6   The license of the main car in the image is C...   3.328562          288   \n",
       "7                                    Madame Tussauds   2.775198          276   \n",
       "8   The diagram explains the concept of contrasti...   5.207717          582   \n",
       "\n",
       "   answer_correctness  \n",
       "0            0.613849  \n",
       "1            0.919710  \n",
       "2            0.744702  \n",
       "3            0.518296  \n",
       "4            0.988251  \n",
       "5            0.993904  \n",
       "6            0.717002  \n",
       "7            0.974335  \n",
       "8            0.688275  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"eval_results/gemini_evals.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run streamlit_dashboard.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
